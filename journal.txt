1. How to mock coroutines: https://stackoverflow.com/questions/29881236/how-to-mock-asyncio-coroutines
2. Python's super method for multiple inheritance: https://stackoverflow.com/questions/3277367/how-does-pythons-super-work-with-multiple-inheritance
3. Building async web sockets in Python: https://www.piesocket.com/blog/python-websocket

Problem with testing coroutines via mock.
-----------------------------------------
1. Our web socket is async so it is awaited. 
2. Normal mock functions wont work. So we used async mock.
3. Sending mock.AsyncMock will return a class which can further be used to create mock coroutines.
    Therefore at first we create the mock handler.
        - self.mock_handler: mock.AsyncMock = mock.AsyncMock
    This serves as the websocket socket handler method.
    Since it is not a coroutine yet, we can add a dummy coroutine method.
        - self.mock_handler.recv: mock.AsyncMock = mock.AsyncMock()
    So now we have succesfully mocked the websocket recv coroutine a mock coroutine handler.
    Now we need to set the return_value for the mock coroutine.
        - self.mock_handler.recv.return_value = "test_message"
    We can change this return value and test the results accordingly.
4. Now when we run our application method with asyncio.run, we will get an exception.
    This is because of course we have mocked the websocket and no actual messages are received.
    So instead, we handle the exception and test if expected calls where made within the exception handling.
    this is done in unit test.

Problem with testing coroutines without mock, in integration tests.
-------------------------------------------------------------------
1. We need to test response from web servers.
2. This means the web server needs to run, while the client makes requests to it.
3. Both web server and the client needs to run.
4. We tried the following option at first:
    - So first we have the web socket server.
    - We create a client coroutine which sends messages to the web socket server.
    - The nature of coroutines is that only one of them can run in non blocking fashion,
        the other one will run only after the previous one is over.
    - So if we start the web server, there is no way that we can run the client coroutine.
5. Created non blocking tasks.
    - Tasks however run concurrently.
    - So we create a web server task and client tasks.
        - Client tasks are basically, the client coroutine run with different messages.
        - To be exact, we have 5 client tasks, i.e. 5 different messages sent with the  same coroutine.
    - The idea is to run the web server task which will keep running.
    - Then run the client tasks until they are complete, since tasks can run concurrently without the previous one having to end.
    - Once the client tasks are done, we close the web server.
6. Problem:
    - The test case does not run because it says no running event loop. For this, we looked up online:
        - We came accross this: https://stackoverflow.com/questions/58774718/asyncio-in-corroutine-runtimeerror-no-running-event-loop
        - This suggested us to create tasks with the async event loop like: asyncio.get_event_loop().create_task(coroutine_function(arg1, ...)),
            instead of plain old asyncio.create_task(coroutine_function(arg1, ...)).
        - This resolved the no running event loop issue, we created tasks with event loop and appended them in a list. Then we used,
            asyncio.gather to run them. But then we came accross another issue, the code runs but we get, task was destroyed but it was pending.
        - Even when we get the res and call res.result(), we dont get anything, it just says error state, result is pending for the futures.
        - Since asyncio.gather returns future objects, we get futures whose results are pending.
        - Even when we await their results, were not getting anything.
    - To solve this issue we tried.
        - We came accross this: https://stackoverflow.com/questions/52691931/simple-way-to-test-websocket-availability-in-python
        - Here we found a method called `async_to_sync`. However, this post assumes that the web server is already running.
        - We still have to run the web server in case of our code. Then we need to run the client in the unittest methods.
    - We tried an intermediate thing:
        - In the test case, instead of asyncio.run, we used asyncio.get_event_loop().run_until_complete(test_everything()).
        - Here we got the exception, asyncio.exceptions.InvalidStateError: Result is not set.
        - Atleast we got rid of the task was destroyed but it was pending error.
    - To solve this issue as well, we followed this tutorial to learn how to use asyncio.gather: https://superfastpython.com/asyncio-gather/
        - Here we saw that we await asyncio.gather(*tasks) rather than just run this assign it to a variable and await later, that doesnt work.
        - However we got, This event loop is already running error. Now we should fix this and things should work.
    - We found this post: https://stackoverflow.com/questions/46827007/runtimeerror-this-event-loop-is-already-running-in-python
        - This post also uses asyncio.get_event_loop().run_until_complete(foo()) multiple times and asks the same question we did.
        - Why is this a problem.
7. We will go with the following approach for this:
    - We will run the backend container which will act as the webserver backend container.
    - We will then create socket clients and send the requests and test the responses. This will be the final test.

Problem with running the commands from the container:
-----------------------------------------------------
1. Our application is dockerized. It needs to run the commands on the host machine rather than within itself.
2. Within the container itself there is no docker daemon, we can connect to a different daemon.
    It is possible to execute docker commands with daemon address. We will need to find resources for this method as well.
    Okay we found it, we can do it with -H or --host option. This will let us select the docker daemon.
    We can check this by doing docker --help. We will see -H option in the options section.
    However, this is not what we will be doing.
3. However, for now we need to pipe the container to the host machine and make sure commands are executed to the host machine.
4. For this we found the following: https://medium.com/@andreacolangelo/sibling-docker-container-2e664858f87a
5. However, we are going with a different approach.
    - We can either create a container within the container but it is considered bad practice.
    - So the basic idea is to not create a container within the container but rather create a sibling container outside the container.
6. This is possible my mapping the docker daemon of the host machine to the docker daemon inside the container.
    This is possible by using volumes.
    In all unix machines, the docker daemon is located at /var/run/docker.sock, we need to map it with the docker.sock of the container,
    which is also located at /var/run/docker.sock.
    So we need to map a volume like -v /var/run/docker.sock:/var/run/docker.sock
7. Since we are using the host daemon, all docker commands within the container will be executed by the host daemon and things should work.

Problem with exec not running:
------------------------------
1. When we were running docker exec -it instead of getting the results we were getting an empty list and
    an error on docker saying that input device is not a tty.
2. We looked online and found this: https://stackoverflow.com/questions/43099116/error-the-input-device-is-not-a-tty
3. This says that we remove -it option and run the command and we should get the result.
4. After removing the command, we had to refactor the test cases for both unit and integration test.
5. Also, when running the tests we found that using docker stop and then using rm would not delete the container, it would only stop it.
6. Because of this recreating the contiainer in integration tests was causing errors and the tests were failing.
7. Therefore we removed the docker stop command and directly added rm -f, which solved the problem and our tests passed.


Enabling SSH:
-------------
1. SSH has to be enabled inside the docker container. Once that is done,
    we need to be able to ssh into the container using python.
    Then all responses shall be tested.

    Dockerfile for centos enabled with ssh: https://gist.github.com/lenchevsky/7eba11bd491e70105de3600ec9ec1292

    Check: How to test if ssh is running in the container: 
    https://cplusprogrammer.wordpress.com/2016/10/17/how-to-check-if-ssh-is-running-on-linux/

    Here is an ubuntu container that acts as a ssh server: https://dev.to/s1ntaxe770r/how-to-setup-ssh-within-a-docker-container-i5i
    The user is test and the password is test.
    We ssh using: ssh test@0.0.0.0 -p 22 and the password is test as well.

2. Now we need to create a Python session and check the responses. We will
    check for the following commands:
    - top: should give us the top ui.
    - mkdir, cd and ls: should create, enter and list directories.

3. So we are able to create SSH connections with Python, but the same thing happens
    we are still unable to get a proper session running.
    So we need to build the session ourselves.


Building terminal session:
---------------------------
1. The terminal keeps track of the current working directory and executes commands accordingly
2. We need to detect directories in the command.
    A regular expression will be used for this.
3. Once that is done we need to to construct the absolute path with the root.

Do we really need SSH?
-----------------------
1. Consider top command. If we use docker exec we cant get the output of top.
2. But we can however get top using -b. 
3. So is every command unique? No. it shouldnt be.
4. Lets just try figuring out cd first and then everything else.
<<<<<<< HEAD


FINALLY THE SOLUTION:
---------------------
1. Before every command, we will do a cd to current working directory.
2. The current working directory needs to be updated via every cd command.
3. Thats it.

We will do a docker exec -w command. Every time.
